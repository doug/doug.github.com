---
layout: article
title: Lying to Learn
author: Doug Fritz & Brian Allen
description: 'Here we have designed anonlinechatgame whose main purpose is to collect common‐sense knowledge by meansstructuredlying. Its focus, is on on the often‐ignored realm of half‐truths and associations latent in a good lie. We posit that learning the truth may be improved by understanding the good lies that surround it. Learning these different connective ways of thinking may proveto be a particularly powerful avenue to explore for developing better common‐sense knowledge‐bases.'
---

Introduction 
-------

In the online game Verbosity, users play word‐guessing games similar to the popular party game Taboo, but with a twist – all of their responses are recorded and archived. The idea is to create an enormous corpus of “common‐sense” knowledge from mining the users’ responses, and ultimately use this information to make computers smarter and more useful. Verbosity excels at this task because it can collect a large amount of data very fast, by virtue of its interactive game‐play and massive user community, unlike other systems designed for common‐sense knowledge acquisition that require tedious, manual input into a database (Von Ahn, Kedia et al. 2006). Here we have designed a game similar in concept, in that its main purpose is to collect common‐sense knowledge by means of a multiplayer‐online game. Its focus, however, on on the often‐ignored realm of half‐truths and associations latent in a good lie. We posit that learning the truth may be improved by understanding the good lies that surround it. Learning these different connective ways of thinking may prove to be a particularly powerful avenue to explore for developing better common‐sense knowledge‐bases. 

Quality and diversity of data 
--------

Though we have chosen to create a multiplayer online game in some ways similar to Verbosity, the data collected explores more fully the realm of different type of connected concepts and associations, rather than common‐sense attachments to data, it begins to explore how we reason about data. By ignoring the current problem of NLP parsing of results and focusing on quality and diversity of data collection we are able to optimize our results toward the wide array of goal oriented stories humans exchange everyday. In this process, rather than relying on the relatively slim demographic who chooses to manually input contrived bits of common‐sense knowledge into a database – one imagines this is laden with computer science graduate students ‐‐ we will be exploiting the much more demographically‐diverse online gaming community. Perhaps this could lead to not only a more diverse corpus of common‐sense knowledge, but could also give us a sense of how such knowledge is distributed across the population. For an example, is common‐sense knowledge as classified by other projects such as ConceptNet truly common to all people? Is it bimodally distributed, such that half of the population has it, while the other half doesn’t? 

Game design
----------

Our game, The Confabulator, is a modified implementation of the party game Malarky, designed as a multiplayer‐online game. In the game, players are asked to respond to a question like “Why is our little finger called a pinkie?” with an explanation. One of the players will be randomly given the actual answer, which they must then rephrase and submit. The players then vote on the explanation that seems most plausible. Responses of the players are unconstrained, i.e. not required to follow a particular template. This differs from the design philosophy of Verbosity, in which response templates are to be completed by the player, to ultimately allow for easier interpretation of the responses by a natural language processor (Von Ahn, Kedia et al. 2006). Here we have chosen to create a game that imposes minimal constraints on real‐world interaction, in hopes of capturing the types of common‐sense knowledge that are pertinent in natural, fluid communication between people – the scenarios that are important for human‐computer interaction. While this makes for a harder problem for the A.I. parser, we believe the trade‐off between the richness of the data set and potentially having to wait a few years for continued progress in natural language processing to fully classify the data falls in favor of the former. This is representative of our guiding design principle – to create a fun game for massive data collection along the lines of Verbosity, but to capture richer and more interesting interactions and types of data. This is not to say that our structure is without constraints, but that those constraints are focused on constraining the context of the response by means of gameplay goals rather than explicit templates. By structuring the type of responses that result in winning, we can organize the responses along the continuum of their resulting points without explicit classification of the data itself. We can measure and evaluate relations between data by understanding a response’s resultant effect on the social system. Resultantly, we have chosen to design a game such that we can capture as much useful data as possible. This leads to the inevitable questions of how to store and organize the data. At this stage, we have chosen to store every bit of information that we have deemed relevant about the game‐play in a relational database – where pretty much every variable related to game‐play is deemed potentially relevant for future analyses. Data recorded includes the questions an individual has seen, players he has competed against, his performance, the timing of his responses, etc.  No preprocessing steps, of averaging or lumping of individuals’ responses, take place before they are entered into the database. Therefore, we can track an individual’s performance, behavior, and interactions with other players.  Another design consideration is how to group people together to play the game. From the standpoint of data acquisition, collecting purely random groups of people together to play the game might be advantageous, as the data generated by a diverse group would likely yield a better sense of which concepts are truly common sense. However, in order to keep players interested enough in the game to continue coming back to play more, the game would ideally adjust to the player’s skill level – i.e. expert players would be matched with other expert players and so on. Additionally, players in a competitive atmosphere may be more motivated to contribute “serious” responses, whereas those playing with people beneath their skill level may start to enter spurious responses to be patronizing, etc. To this concern, data such as point totals are collected and made known to the user so that they could be used to match groups of people at a particular skill level. 

Real‐world interaction
---------

In the spirit of capturing realistic, rich interactions, we’ve chosen to minimize constraints on the natural timing, or “flow” of the game. As such, a player presents his explanation as soon as he arrives at it, and sees the explanations of others as they stream in. An additional chat feature allows for social communication among players during this process. Other avenues to explore might include introducing a short time limit for answering, playing with the notion that when people are rushed, they may respond in a more “raw”, i.e. less‐contrived, way. This likely would have a detrimental effect on the overall “quality” of responses, but could tell us something about the concepts that are invoked during peoples’ gut reaction to particular questions. One imagines this might not only be interesting from a psychological standpoint, but could also inform a computer how to behave more humanistically when being put on the spot. 

Rules of the game
-------

A question is asked. One player is randomly chosen to receive the answer to that question. If you were given the answer to the question, you must restate the answer in your own words, unless someone else responses with the answer before you. Each explanation/bluff is submitted, and if someone gives an explanation that is identical to what you were going to say before your turn, you must give a different response, even if your explanation is correct (repeated explanations are not allowed). After everyone has given an explanation, each player votes on which seems the most correct. After all the votes have been submitted, both the real answer and its author are revealed. You get one point for voting for the correct answer. You get a point for every vote your answer gets even if it was wrong. Each game is played until a player reaches 10 points. 

Motivating idea I: Lying and concept clustering
----------

Lying in the process of explanation is interesting for a number of reasons from a knowledge representation standpoint. When someone tries to lie convincingly, they typically do so with an expression that is close enough to the truth to be believable, but isn’t the truth per se. The inter‐relatedness of concepts is a difficult problem for knowledge representation – are two concepts similar because they share similar elements? Are they similar because they share common relationships? How are concepts actually clustered in the mind? In our game, concept clustering relies on the voting of the players – the explanations they vote for with higher‐than‐chance frequency share elements common to or evoke ideas common with the content of the question itself. This process allows us to sidestep the currently under‐mapped world of concepts in natural language. In its current form, information generated through the game can be clustered in this way, but ultimately natural language processing will have to map these explanations to actual concepts. That task is however, outside the scope of this initial paper and will thus be deferred. While no doubt useful representations of concepts, or concept maps, can be generated through techniques that apply logical operations on elements and relationships, it’s likely that this reductionist strategy does not capture many of the complexities inherent in human knowledge representation. Consider the following, which is a real world example of Confabulator where the conceptual reasons given are clearly interrelated, but not naively so. There is currently no corpus of significant size containing this type of data available to researchers. 	Confabulator: Why is a pedestrian violation called ‘jaywalking’?

* Player 1: They used to throw people in jail for it, and was originally referred to as ‘jailwalking’, but later became known as ‘jaywalking’. 
* Player 2 (was given reason and reworded): jay is a term for hick related to bluejays and hick‐walking refers to how hick walk in cities, without respect for signs. 
* Player 3: ‘jaywalking’ is crossing not at the crosswalk, and birds, like bluejays don’t follow signs. 

Motivating idea II: Explanation and common‐sense knowledge
-------

Whereas we can create concept clusters through the analysis of lies, The Confabulator can also judge whether or not a particular concept is in the realm of common‐sense knowledge, by how many people recognize the correct explanation as such. This is possible because one player is always given the correct explanation to present – the person chosen changes randomly from round‐to‐round. Thus players are unaware of who has given the correct answer a priori each round, but will always have the opportunity to vote for an answer that is right, regardless of how well those playing are able to generate the proper explanation for the question. Voting at the end of each round can be used as a metric for common knowledge as illustrated by the following example: 	Case 1: If everyone votes for Answer 1 ‘A’, then we know x happened because of A, and that this is common knowledge. Plus, when know B,C,D are not the cause of X, but could and probably are tangentially related, however with an obviously correct answer the likelihood of the other responses just trying to being funny goes way up. 	Case 2: If the votes are more distributed, i.e. 2 for A, and 1 for B and 1 for C. One, we know that the answer is less likely to be common knowledge, and that B and C are related to X. Players are probably less likely to have made humorous remarks. Case 3: If everyone votes for themselves we know very little but can still probably assume that all the responses are in some way related to the original question though not the direct cause, and the question itself is not very common. Mechanistic versus functional explanation, simplicity, and authority Besides using the large corpus of data generated with The Confabulator to explore common sense and concept clustering, a meta‐analysis of the kinds of explanations and explanation strategies employed can be performed. Explanation is inextricably linked to how people categorize – and the relative importance of features of a concept can be manipulated through whether a person has learned about them as being functionally or mechanistically derived. Observing whether a player responds to a question such as “why are tigers striped” by appealing to, say, its pigmentation (mechanistic) versus the fact that it provides good camouflage (functional) (Lombrozo 2008), and whether or not his style of answering is stereotyped across many trials would be interesting from the standpoint of psychology. Are some people more prone to functional explanations than others? Or do some people appeal more to mechanistic versus functional explanations in particular domains of knowledge? Additionally, given the data corpus from The Confabulator, one could analyze the particular features of an explanation that make it most believable. Psychological studies suggest that simple explanations in a particular behavioral paradigm require a disproportionately less amount of probabilistic evidence relative to more complex explanations (Lombrozo 2007); people seem to be applying Occam’s Razor even in the presence of significant disproving evidence. How generalizable is this phenomenon? What factors can come into play to outweigh the allure of simple explanations?  Another interesting question that could be addressed with the corpus of data from The Confabulator is: do certain people appeal to arguments from authority more than others? For an example, when speaking of a scientific result, does a particular person often cite the name of the prestigious university when appealing to a study’s credibility, or does he typically ignore those kinds of details and focus solely on the perceived merits of the science? Similarly, how does the inclusion of “impressive sounding” terms and concepts affect the believability of an explanation for various people? For an example, does including neuroscience data in an explanation of a psychological concept, regardless of its relevance to the question at hand, tend to make the explanation more believable? Preliminary evidence from psychology points to yes (Trout 2008), but how generalizable is this phenomenon, and how robust is it when presented to people of varying backgrounds and personalities? With the data corpus generated using The Confabulator, questions like these could be addressed in a comprehensive way for the first time.    

The psychology of lying
------------

While thus far we have emphasized the usefulness of having a game of lying for the purposes of concept clustering and for delving into the psychological nature of explanation, much about lying as a thing in itself could also be learned from the vast swaths of data collected by The Confabulator. Do some people routinely lie in predictable ways? Do humans in general do certain stereotyped things to make things sound more believable? How are lying strategies and the structure of lies context and goal‐dependent? Answers to these questions are essentially absent in psychological literature – the small scale of psychological studies, lack of generalizability of many lab experiments, and the lack of focus on lying with respect to knowledge representation have limited what we have been able to learn about lying. The Confabulator could make a big impact on this relatively barren corner of psychology. Implementation details Python, Jabber, Gchat, SQLite, xmpppy Confabulator was built as a python chat bot, backed by a sql database. Individuals join games through which all behavior is tracked and recorded for later use in datamining and learning. 

Future Work
------------

Given the optimization toward ease of use (Google Chat’s ubiquity) and The Confabulator’s highly engaging gameplay, we feel confident in being able to rapidly attract large sets of users and generate a corpus for researchers that should prove both novel and highly applicable in the future development of intelligent systems. While the initial implementation makes use of static questions and answers, future versions, could answer their own questions, by having a signification participating body, and posing the same questions simultaneously to multiple groups.  As the corpus grows, we foresee small‐scale studies, becoming a thing of the past and data mining these sort of large‐scale corpuses with a specification intention becoming the path of research in the future. Ultimately, humans tell stories to learn and to play, whether they are fact or fiction, we construct our world through a series of oddly stringed together bits of knowledge and exposing that process will hopefully elucidate knew incites into the human condition. 